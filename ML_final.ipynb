{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 언더샘플링 기반 LightGBM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading preprocessed dataset...\n",
      "✅ Dataset loaded: 41188 rows, 43 columns\n",
      "🧪 Train shape: (28831, 42), Validation shape: (12357, 42)\n",
      "🔍 Running GridSearchCV (scoring = F2-score)...\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "[LightGBM] [Info] Number of positive: 3248, number of negative: 4060\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 418\n",
      "[LightGBM] [Info] Number of data points in the train set: 7308, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444444 -> initscore=-0.223144\n",
      "[LightGBM] [Info] Start training from score -0.223144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "📊 Evaluation Metrics on Validation Set\n",
      "Accuracy     : 0.8657\n",
      "Precision    : 0.4329\n",
      "Recall       : 0.6185\n",
      "F1 Score     : 0.5093\n",
      "F2 Score     : 0.5697\n",
      "ROC AUC Score: 0.8100\n",
      "\n",
      "📄 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92     10965\n",
      "           1       0.43      0.62      0.51      1392\n",
      "\n",
      "    accuracy                           0.87     12357\n",
      "   macro avg       0.69      0.76      0.72     12357\n",
      "weighted avg       0.89      0.87      0.88     12357\n",
      "\n",
      "\n",
      "🔍 Confusion Matrix:\n",
      "[[9837 1128]\n",
      " [ 531  861]]\n",
      "\n",
      "💾 Best model saved to: C:/ITStudy/bank-marketing/model\\lgbm_under_f2.pkl\n"
     ]
    }
   ],
   "source": [
    "# ▒▒ 1. 필수 라이브러리 임포트 ▒▒\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.pipeline import Pipeline  # ✅ imblearn Pipeline 사용\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, fbeta_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix,\n",
    "    make_scorer  # ✅ F2-score용\n",
    ")\n",
    "\n",
    "# ▒▒ 2. 전처리 완료된 데이터 로드 ▒▒\n",
    "print(\"📂 Loading preprocessed dataset...\")\n",
    "df = pd.read_csv('C:/ITStudy/bank-marketing/data.csv')\n",
    "print(f\"✅ Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# ▒▒ 3. 데이터 분할 ▒▒\n",
    "X = df.drop(columns='y')\n",
    "y = df['y']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"🧪 Train shape: {X_train.shape}, Validation shape: {X_val.shape}\")\n",
    "\n",
    "# ▒▒ 4. 언더샘플링 + LightGBM 파이프라인 구성 ▒▒\n",
    "pipeline = Pipeline([\n",
    "    ('undersample', RandomUnderSampler()),\n",
    "    ('lgbm', LGBMClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# ▒▒ 5. 하이퍼파라미터 그리드 정의 ▒▒\n",
    "param_grid = {\n",
    "    'undersample__sampling_strategy': [0.5, 0.8],\n",
    "    'lgbm__n_estimators': [50, 100, 200],\n",
    "    'lgbm__max_depth': [-1, 3, 5],\n",
    "    'lgbm__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'lgbm__subsample': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# ▒▒ 6. GridSearchCV 실행 (F2-score 기준) ▒▒\n",
    "print(\"🔍 Running GridSearchCV (scoring = F2-score)...\")\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring=f2_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ▒▒ 7. 최적 모델 예측 수행 ▒▒\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_prob = best_model.predict_proba(X_val)[:, 1]\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# ▒▒ 8. 평가 지표 함수 정의 및 출력 ▒▒\n",
    "def print_metrics(y_true, y_pred, y_score):\n",
    "    print(\"\\n📊 Evaluation Metrics on Validation Set\")\n",
    "    print(f\"Accuracy     : {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Precision    : {precision_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Recall       : {recall_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"F1 Score     : {f1_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"F2 Score     : {fbeta_score(y_true, y_pred, beta=2):.4f}\")\n",
    "    print(f\"ROC AUC Score: {roc_auc_score(y_true, y_score):.4f}\")\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"\\n🔍 Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print_metrics(y_val, y_pred, y_pred_prob)\n",
    "\n",
    "# ▒▒ 9. 최종 모델 저장 ▒▒\n",
    "model_dir = \"C:/ITStudy/bank-marketing/model\"\n",
    "model_path = os.path.join(model_dir, \"lgbm_under_f2.pkl\")\n",
    "\n",
    "# 디렉토리 없으면 생성\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# 모델 저장\n",
    "with open(model_path, \"wb\") as model_file:\n",
    "    pickle.dump(best_model, model_file)\n",
    "\n",
    "print(f\"\\n💾 Best model saved to: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost 언더샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading preprocessed dataset...\n",
      "✅ Dataset loaded: 41188 rows, 43 columns\n",
      "🧪 Train shape: (28831, 42), Validation shape: (12357, 42)\n",
      "🔍 Running GridSearchCV (scoring = F2-score)...\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dahee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [14:22:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Metrics on Validation Set\n",
      "Accuracy     : 0.8647\n",
      "Precision    : 0.4303\n",
      "Recall       : 0.6214\n",
      "F1 Score     : 0.5085\n",
      "F2 Score     : 0.5707\n",
      "ROC AUC Score: 0.8114\n",
      "\n",
      "📄 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92     10965\n",
      "           1       0.43      0.62      0.51      1392\n",
      "\n",
      "    accuracy                           0.86     12357\n",
      "   macro avg       0.69      0.76      0.72     12357\n",
      "weighted avg       0.89      0.86      0.88     12357\n",
      "\n",
      "\n",
      "🔍 Confusion Matrix:\n",
      "[[9820 1145]\n",
      " [ 527  865]]\n",
      "\n",
      "💾 Best model saved to: C:/ITStudy/bank-marketing/model\\xgb_under_f2.pkl\n"
     ]
    }
   ],
   "source": [
    "# ▒▒ 1. 필수 라이브러리 임포트 ▒▒\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, fbeta_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "# ▒▒ 2. 전처리 완료된 데이터 로드 ▒▒\n",
    "print(\"📂 Loading preprocessed dataset...\")\n",
    "df = pd.read_csv('C:/ITStudy/bank-marketing/data.csv')\n",
    "print(f\"✅ Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# ▒▒ 3. 데이터 분할 ▒▒\n",
    "X = df.drop(columns='y')\n",
    "y = df['y']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"🧪 Train shape: {X_train.shape}, Validation shape: {X_val.shape}\")\n",
    "\n",
    "# ▒▒ 4. 언더샘플링 + XGBoost 파이프라인 구성 ▒▒\n",
    "pipeline = Pipeline([\n",
    "    ('undersample', RandomUnderSampler(random_state=42)),\n",
    "    ('xgb', XGBClassifier(\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',      # XGBoost 경고 제거\n",
    "        tree_method='hist'          # ❗ GPU 대신 CPU 기반 학습 설정\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ▒▒ 5. 하이퍼파라미터 그리드 정의 ▒▒\n",
    "param_grid = {\n",
    "    'undersample__sampling_strategy': [0.5, 0.8],\n",
    "    'xgb__n_estimators': [50, 100, 200],\n",
    "    'xgb__max_depth': [3, 5, 8],\n",
    "    'xgb__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'xgb__subsample': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# ▒▒ 6. GridSearchCV 실행 (F2-score 기준) ▒▒\n",
    "print(\"🔍 Running GridSearchCV (scoring = F2-score)...\")\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring=f2_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ▒▒ 7. 최적 모델 예측 수행 ▒▒\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_prob = best_model.predict_proba(X_val)[:, 1]\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# ▒▒ 8. 평가 지표 함수 정의 및 출력 ▒▒\n",
    "def print_metrics(y_true, y_pred, y_score):\n",
    "    print(\"\\n📊 Evaluation Metrics on Validation Set\")\n",
    "    print(f\"Accuracy     : {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Precision    : {precision_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Recall       : {recall_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"F1 Score     : {f1_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"F2 Score     : {fbeta_score(y_true, y_pred, beta=2):.4f}\")\n",
    "    print(f\"ROC AUC Score: {roc_auc_score(y_true, y_score):.4f}\")\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"\\n🔍 Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print_metrics(y_val, y_pred, y_pred_prob)\n",
    "\n",
    "# ▒▒ 9. 최종 모델 저장 ▒▒\n",
    "model_dir = \"C:/ITStudy/bank-marketing/model\"\n",
    "model_path = os.path.join(model_dir, \"xgb_under_f2.pkl\")\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "with open(model_path, \"wb\") as model_file:\n",
    "    pickle.dump(best_model, model_file)\n",
    "\n",
    "print(f\"\\n💾 Best model saved to: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
